{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c01f7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "bbc_to_csv.py\n",
    "\n",
    "Fetch BBC News across all categories, parse each article for metadata\n",
    "and full content, then dump everything into bbc_news.csv.\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import bbc_feeds\n",
    "import pandas as pd\n",
    "\n",
    "# --- Constants ---\n",
    "HEADERS = {\"User-Agent\": \"bbc-news-scraper/1.0 (+https://yourdomain.example)\"}\n",
    "CATEGORIES = [\n",
    "    \"all\", \"world\", \"uk\", \"north_america\",\n",
    "    \"entertainment\", \"business\", \"tech\", \"science\"\n",
    "]\n",
    "CSV_PATH = \"bbc_news.csv\"\n",
    "FETCH_LIMIT = 500   # adjust as needed\n",
    "REQUEST_DELAY = 1   # seconds between requests\n",
    "\n",
    "def parse_article(url):\n",
    "    \"\"\"Fetches URL and extracts title, section, author, publish time, and content.\"\"\"\n",
    "    resp = requests.get(url, headers=HEADERS)\n",
    "    soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "\n",
    "    # Headline\n",
    "    print(\"title: \", title)\n",
    "    title_tag = soup.find(attrs={'data-component': 'headline-block'})\n",
    "    title = title_tag.get_text(strip=True) if title_tag else None\n",
    "\n",
    "    # Section & Author\n",
    "    section_meta = soup.find('meta', property='article:section')\n",
    "    author_meta  = soup.find('meta', property='article:author')\n",
    "    section = section_meta['content'] if section_meta else None\n",
    "    author  = author_meta['content']  if author_meta  else None\n",
    "\n",
    "    # Publish Time\n",
    "    pub_meta = soup.find('meta', property='article:published_time')\n",
    "    if pub_meta and pub_meta.get('content'):\n",
    "        published = pub_meta['content']\n",
    "    else:\n",
    "        time_tag = soup.find('time')\n",
    "        published = time_tag['datetime'] if (time_tag and time_tag.has_attr('datetime')) else None\n",
    "\n",
    "    # Full text\n",
    "    paragraphs = []\n",
    "    for block in soup.find_all('div', {'data-component': 'text-block'}):\n",
    "        for p in block.find_all('p'):\n",
    "            paragraphs.append(p.get_text(strip=True))\n",
    "    content = \"\\n\\n\".join(paragraphs) if paragraphs else None\n",
    "\n",
    "    return {\n",
    "        \"title\":     title,\n",
    "        \"section\":   section,\n",
    "        \"author\":    author,\n",
    "        \"published\": published,\n",
    "        \"url\":       url,\n",
    "        \"content\":   content\n",
    "    }\n",
    "\n",
    "def fetch_all_categories(limit=FETCH_LIMIT):\n",
    "    \"\"\"\n",
    "    Fetches up to `limit` stories from each BBC category and\n",
    "    returns a list of article-info dicts.\n",
    "    \"\"\"\n",
    "    client = bbc_feeds.news()\n",
    "    all_articles = []\n",
    "\n",
    "    for cat in CATEGORIES:\n",
    "        stories = getattr(client, cat)(limit=limit)\n",
    "        for story in stories:\n",
    "            # Basic RSS info\n",
    "            info = {\n",
    "                \"rss_category\": cat,\n",
    "                \"rss_title\": story.title,\n",
    "                \"rss_link\": story.link\n",
    "            }\n",
    "            # Fetch and parse the article itself\n",
    "            try:\n",
    "                article_data = parse_article(story.link)\n",
    "            except Exception as e:\n",
    "                article_data = {\n",
    "                    \"title\": None, \"section\": None, \"author\": None,\n",
    "                    \"published\": None, \"url\": story.link, \"content\": None\n",
    "                }\n",
    "            # Merge RSS + parsed data\n",
    "            info.update(article_data)\n",
    "            all_articles.append(info)\n",
    "\n",
    "            time.sleep(REQUEST_DELAY)  # polite crawling\n",
    "\n",
    "    return all_articles\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Fetch everything\n",
    "    articles = fetch_all_categories()\n",
    "\n",
    "    # 2. Convert to DataFrame\n",
    "    df = pd.DataFrame(articles)\n",
    "\n",
    "    # 3. Save to CSV\n",
    "    df.to_csv(CSV_PATH, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Saved {len(df)} articles to {CSV_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24d69e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"bbc_news.csv\")\n",
    "\n",
    "df.iloc[1][\"content\"]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
