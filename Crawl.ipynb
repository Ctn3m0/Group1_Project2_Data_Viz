{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c01f7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Fetch BBC News across all categories, then visit each article\n",
    "and extract headline, description, section, content_topic, authors,\n",
    "publish & modify times, image URLs, and full content,\n",
    "then export all results into a CSV file, with a progress bar for each category.\n",
    "Improved network resilience via retries/backoff to avoid SSL/network errors.\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import csv\n",
    "import random\n",
    "import requests\n",
    "from requests.exceptions import RequestException, SSLError\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util import Retry\n",
    "from bs4 import BeautifulSoup\n",
    "import bbc_feeds\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Configure a session with retries/backoff ---\n",
    "session = requests.Session()\n",
    "retries = Retry(\n",
    "    total=5,\n",
    "    backoff_factor=0.5,\n",
    "    status_forcelist=[429, 500, 502, 503, 504],\n",
    "    allowed_methods=[\"GET\"],\n",
    "    raise_on_status=False\n",
    ")\n",
    "adapter = HTTPAdapter(max_retries=retries)\n",
    "session.mount('https://', adapter)\n",
    "session.mount('http://', adapter)\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"bbc-news-scraper/1.0 (+https://yourdomain.example)\"\n",
    "}\n",
    "\n",
    "OUTPUT_CSV = \"bbc_news_articles.csv\"\n",
    "\n",
    "\n",
    "def parse_article(url):\n",
    "    try:\n",
    "        resp = session.get(url, headers=HEADERS, timeout=10, verify=True)\n",
    "        resp.raise_for_status()\n",
    "    except (RequestException, SSLError) as e:\n",
    "        print(f\"[Error] Failed to fetch {url}: {e}. Retrying... \")\n",
    "        # final retry after backoff\n",
    "        try:\n",
    "            time.sleep(2)\n",
    "            resp = session.get(url, headers=HEADERS, timeout=20, verify=True)\n",
    "            resp.raise_for_status()\n",
    "        except Exception as e2:\n",
    "            # Give up after retries\n",
    "            print(f\"[Fatal] Could not fetch {url} after retries: {e2}\")\n",
    "            return None\n",
    "\n",
    "    soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "\n",
    "    # --- 1. Grab JSON-LD if present ---\n",
    "    data = {}\n",
    "    ld = soup.find(\"script\", type=\"application/ld+json\")\n",
    "    if ld and ld.string:\n",
    "        try:\n",
    "            data = json.loads(ld.string)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "\n",
    "    # --- Try to pull BBC dotcomConfig JSON for section & topics ---\n",
    "    bbc_cfg = {}\n",
    "    cfg_script = soup.find('script', id='inline-dotcom-config')\n",
    "    if cfg_script and cfg_script.string:\n",
    "        match = re.search(r\"window\\.dotcomConfig\\s*=\\s*(\\{.*\\});\", cfg_script.string, re.DOTALL)\n",
    "        if match:\n",
    "            try:\n",
    "                bbc_cfg = json.loads(match.group(1))\n",
    "            except json.JSONDecodeError:\n",
    "                bbc_cfg = {}\n",
    "\n",
    "    props = bbc_cfg.get('pageData', {}).get('pageProps', {})\n",
    "\n",
    "    # --- 2. Headline & Description ---\n",
    "    headline = data.get(\"headline\") or props.get('page_title') or (soup.title.string.strip() if soup.title else \"N/A\")\n",
    "    description = data.get(\"description\") or props.get('description') or ''\n",
    "    if not description:\n",
    "        desc_meta = soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
    "        description = desc_meta[\"content\"] if desc_meta else ''\n",
    "\n",
    "    # --- 3. Section ---\n",
    "    section = data.get(\"articleSection\") or props.get('content_section')\n",
    "    if not section:\n",
    "        sec_meta = soup.find(\"meta\", property=\"article:section\")\n",
    "        section = sec_meta[\"content\"] if sec_meta else 'N/A'\n",
    "\n",
    "    # --- 4. Content topics / keywords ---\n",
    "    content_topic = data.get(\"keywords\") or props.get('content_topic')\n",
    "    if not content_topic:\n",
    "        kw = (soup.find(\"meta\", attrs={\"name\": \"news_keywords\"}) or \n",
    "              soup.find(\"meta\", attrs={\"name\": \"keywords\"}))\n",
    "        content_topic = kw[\"content\"] if kw and kw.has_attr(\"content\") else ''\n",
    "\n",
    "    # --- 5. Authors (string) ---\n",
    "    authors_list = []\n",
    "    auth = data.get(\"author\")\n",
    "    if auth:\n",
    "        if isinstance(auth, list):\n",
    "            for a in auth:\n",
    "                if isinstance(a, dict) and a.get(\"name\"):\n",
    "                    authors_list.append(a[\"name\"])\n",
    "                else:\n",
    "                    authors_list.append(str(a))\n",
    "        elif isinstance(auth, dict) and auth.get(\"name\"):\n",
    "            authors_list.append(auth[\"name\"])\n",
    "        else:\n",
    "            authors_list = [str(auth)]\n",
    "    else:\n",
    "        if props.get('byline'):\n",
    "            authors_list = [props['byline']]\n",
    "        else:\n",
    "            am = soup.find(\"meta\", property=\"article:author\")\n",
    "            if am and am.has_attr(\"content\"):\n",
    "                authors_list = [am[\"content\"]]\n",
    "    authors = \", \".join(authors_list)\n",
    "\n",
    "    # --- 6. Published & Modified times ---\n",
    "    published = data.get(\"datePublished\") or props.get('date_published') or ''\n",
    "    date_modified = data.get(\"dateModified\") or props.get('date_modified') or ''\n",
    "\n",
    "    # --- 7. Images ---\n",
    "    img = data.get(\"image\", {})\n",
    "    if isinstance(img, dict):\n",
    "        image_url = img.get(\"url\", '')\n",
    "    elif isinstance(img, list) and img:\n",
    "        image_url = img[0].get(\"url\", '')\n",
    "    else:\n",
    "        image_url = props.get('image', '')\n",
    "    if not image_url:\n",
    "        og = soup.find(\"meta\", property=\"og:image\")\n",
    "        image_url = og[\"content\"] if og and og.has_attr(\"content\") else ''\n",
    "\n",
    "    thumbnail_url = data.get(\"thumbnailUrl\") or props.get('thumbnailUrl') or ''\n",
    "    if not thumbnail_url:\n",
    "        tw = soup.find(\"meta\", property=\"twitter:image:src\")\n",
    "        thumbnail_url = tw[\"content\"] if tw and tw.has_attr(\"content\") else ''\n",
    "\n",
    "    # --- 8. mainEntityOfPage ---\n",
    "    main_entity = data.get(\"mainEntityOfPage\") or props.get('mainEntityOfPage') or ''\n",
    "\n",
    "    # --- 9. Full text content ---\n",
    "    paragraphs = []\n",
    "    for block in soup.find_all('div', {'data-component': 'text-block'}):\n",
    "        for p in block.find_all('p'):\n",
    "            paragraphs.append(p.get_text(strip=True))\n",
    "    content = \"\\n\\n\".join(paragraphs)\n",
    "\n",
    "    return {\n",
    "        'headline':      headline,\n",
    "        'description':   description,\n",
    "        'section':       section,\n",
    "        'content_topic': content_topic,\n",
    "        'authors':       authors,\n",
    "        'published':     published,\n",
    "        'modified':      date_modified,\n",
    "        'image_url':     image_url,\n",
    "        'thumbnail_url': thumbnail_url,\n",
    "        'main_entity':   main_entity,\n",
    "        'content':       content,\n",
    "        'url':           url\n",
    "    }\n",
    "\n",
    "\n",
    "def fetch_all_categories(limit=500):\n",
    "    client     = bbc_feeds.news()\n",
    "    categories = [\"all\", \"world\", \"uk\", \"north_america\",\n",
    "                  \"entertainment\", \"business\", \"tech\", \"science\"]\n",
    "\n",
    "    fieldnames = [\n",
    "        'headline', 'description', 'section', 'content_topic',\n",
    "        'authors', 'published', 'modified', 'image_url',\n",
    "        'thumbnail_url', 'main_entity', 'content', 'url'\n",
    "    ]\n",
    "    with open(OUTPUT_CSV, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for cat in categories:\n",
    "            stories = getattr(client, cat)(limit=limit)\n",
    "            print(f\"\\n=== Category: {cat.upper()} ({len(stories)} items) ===\")\n",
    "            for story in tqdm(stories, desc=f\"{cat.upper()}\", unit=\"article\"):\n",
    "                info = parse_article(story.link)\n",
    "                if info:\n",
    "                    writer.writerow(info)\n",
    "                    print(f\"{cat.upper()} -> {info['headline']}\")\n",
    "                # gentle random delay to avoid hammering\n",
    "                time.sleep(random.uniform(1, 3))\n",
    "\n",
    "    print(f\"All data exported to '{OUTPUT_CSV}'\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fetch_all_categories(limit=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24d69e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"bbc_news_articles.csv\")\n",
    "\n",
    "df.iloc[1][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d73fafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81771e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df[df['content'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc99917",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1ddeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# make sure 'published' is a datetime dtype (with UTC)\n",
    "df['published'] = pd.to_datetime(df['published'], utc=True)\n",
    "\n",
    "# get earliest and latest\n",
    "min_pub = df['published'].min()\n",
    "max_pub = df['published'].max()\n",
    "\n",
    "print(f\"Published date range: {min_pub} to {max_pub}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
